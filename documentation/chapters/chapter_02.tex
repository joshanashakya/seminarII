\section{Background Study}
In 2001, Conditional random field was introduced by Lafferty et al. for segmenting and sequencing labeled data. Later, the large margin methods for structured and interdependent output variables \cite{Tsochantaridis2005} were presented. This developed the concept of Structural SVM for label sequence learning, sequence alignment, multi-label classification, etc. Structural SVM for these different problems were implemented by Thorsten Joachims in C, and it was later implemented in python by Andreas Christian Muller.

\section{Literature Review}
A comparison between structural SVM and conventional SVM has been done for reference parsing in \cite{Zhang2010}. The authors compared structural SVM and conventional SVM in token-level and chunk-level reference parsing of medical journal articles using two types of contextual features: observation features and label features. They extracted the observation features from the neighboring tokens and treated the labels assigned to those tokens as the label features. Because structural SVM for reference parsing is implemented as a sequence learning algorithm, and the joint feature presentation function includes two kinds of features: state transition features and observation features extracted from individual tokens within a sequence, the authors used structural SVM designed specially for sequence labeling called SVMHMM. The authors concluded that although SVM performance improved when the second order contextual observation features were used, structural SVM achieved higher overall token-level and chunk-level accuracies than the SVM method. Both methods achieved above 98\% token classification accuracy and an overall chunk-level accuracy of over 95\%.
\\\\
The authors in \cite{B.Chrystal2015} has presented the construction of a classification model in multi-label scenario for the classification of product review documents. Their work dealt with the text classification problem using an approach of multi-label classification using structural support vector machine. They performed the experiment on a collection of product reviews of various electronic gadgets including mobile phones, tablets, laptops, pen-drives, etc. The process was carried out using python implementation of Struct SVM. With 85.4\% accuracy, the authors concluded that the system that they built was an optimized method in the case of a multi-label text classification scenario. In their experiment, they observed that the training time for multi-label classification is considerably high for large datasets and hence they are extending their work with core vector machines considering the scalability aspects of the algorithm.
\\\\
In \cite{Rahmawati2015}, the authors explored the two main approaches in machine learning to solve multi-label classification problem: problem transformation and algorithm adaptation. The writers focused on the four different factors, i.e., feature weighting method, feature selection method, multi-label classification approach, and single-label classification algorithm to obtain the best combination. They performed the experiment using Binary, TF, and TF-IDF for feature selection, Information Gain(IG), Symmetrical Uncertainty(SU), and Correlation Coefficient(CC) for feature selection, problem transformation and algorithm adaptation for multi-label classification, and Na√Øve Bayes, J48, SVM, Adaboost.MH, and MlkNN for single-label algorithm. The result showed that the problem transformation approach, generally, gave better result than algorithm adaptation. Problem transformation approaches that ignore label correlation obtain better performances than methods that consider label correlation because of the low correlation of data used in the experiment. They built a model for automatic multi-label classification of Indonesian news articles using the best combination obtained from the experiments. The combination consists of TF-IDF feature weighting method, Symmetrical Uncertainty feature selection method, Calibrated Label Ranking, which belongs to problem transformation approach, and SVM algorithm. This combination gave F-measure of 85.13\% in 10-fold cross-validation. The F-measure decreased to 76.73\% in testing mode.